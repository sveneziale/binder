{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "In this example we train a neural network from scratch to detect handwritten digits. We use [PyTorch](https://pytorch.org/) (a common deep learning library) to define our network: it allows us to make very explicit the structure of the network and the training loop. Note that neural networks are really useful in *both* supervised and unsuperised machine learning regimes. In this example we use a neural network to address a supervised learning problem.\n",
    "\n",
    "Before looking at this notebook, have a play with [A Neural Network Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.81156&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) which allows you to interactively modify with neural network hyperparameters (e.g. learning rate, network architecture, activation function) and see how changing these parameters affects the outcome. For example:\n",
    "- Note that if you choose your activation function to be 'linear', you can change the other parameters as much as you want, but you'll never be able to learn the first domain (where the inside of the circle is orange and the outside is blue): this is because a circle is non-linear, so it cannot be learned by compositions of linear maps! You need your activation function to add some non-linearity to it. For example if we choose ReLU and keep our learning rate 0.03 then we should be able to learn such a domain.\n",
    "- But we can learn the third domain (the one where orange and blue can be divided by a diagonal line) using a 'linear' activation function: this is because the two domain are linearly separable, so this can be learned by just a composition of linear functions.\n",
    "- Play around with the learning rate: note that if the learning rate is too high the network does not converge, and if it is too low it converges too slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the train-test split of the data\n",
    "train = datasets.MNIST('', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST('', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the data look like? Each datapoint is a 28x28 pixels black and white image of a digit between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out some examples and their labels\n",
    "examples = [train.data[1], train.data[3], train.data[5], train.data[7]]\n",
    "labels = [train.train_labels[1], train.train_labels[3], train.train_labels[5], train.train_labels[7]]\n",
    "\n",
    "# Some boring plotting code\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, examples, labels):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Training label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch both the training and the testing data, for a given batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batch size is an example of an hyperparameter (the dimension of the batch size affects the learning): this is a hyperparameter you can change.\n",
    "batch_size = 500\n",
    "\n",
    "# Batch the training and testing data\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is ready, we need to define a network. We define a network with two hidden layers of 16 neurons each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class defines the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input Layer (we have 28x28 pixels, so the input layer must be of size 28*28, while the next layer is 16 neurons big)\n",
    "        self.input = nn.Linear(28*28, 16)\n",
    "\n",
    "        # Hidden layers (each of them have 16 layers)\n",
    "        self.hidden1 = nn.Linear(16, 16)\n",
    "        self.hidden2 = nn.Linear(16, 16)\n",
    "\n",
    "        # Output layer (we are classifying digits between 0 and 9, so we have 10 classes, hence the output layer must have 10 neurons)\n",
    "        self.output = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We use ReLu activation function on all layers (for other activation functions see https://machinelearningmastery.com/activation-functions-in-pytorch/)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "\n",
    "        # We do not use the ReLU activation function on the output\n",
    "        x = self.output(x)\n",
    "\n",
    "        # We apply log softmax to the output\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define the network\n",
    "net = Net()\n",
    "\n",
    "# Print the network we have just defined (note that this network has all the weights and biases randomly initialised)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both the data and the network we want to train it. We use the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (which is the usual loss function for multi-classification problems) and the Adam optimizer with learning rate `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (this is the quantity we want to minimise during training)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer algorithm\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we go through the running loop, passing through the whole training set multiple times (for the specified number of epochs). Every 100 steps we evaluate on testing to see whether we are overfitting (by printing both the loss on training and testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs (i.e. how many times we are saying the whole of the training data)\n",
    "n_epochs = 10\n",
    "\n",
    "# To record the loss every once in a while\n",
    "losses = []\n",
    "\n",
    "i = 0\n",
    "# Loops over the number of epochs\n",
    "for epoch in range(n_epochs):\n",
    "    # Loop over the data in batches\n",
    "    for data in trainset:\n",
    "        # Upacks features and labels of the data\n",
    "        X, y = data\n",
    "\n",
    "        # Set gradients to zero\n",
    "        net.zero_grad()\n",
    "\n",
    "        # Passes the data through the network\n",
    "        output = net(X.view(-1,28*28))\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        # Backpropagation and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "\n",
    "        # Every 100 steps print the loss for each batch: it should go down if it is learning (and record it for plotting)\n",
    "        if i%100==0:\n",
    "            print(f'Loss at epoch {epoch} and step {i}: ', loss.item())\n",
    "            losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the change in the loss through training by plotting them (this is called learning curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve (training steps against loss)\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we can compute the accuracy on unseen data (the testing data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many are correctly labelled and how many there are in total\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Store the true and the correct labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Store misclassified images\n",
    "misclassified = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        # Get the network output\n",
    "        output = net(X.view(-1,784))\n",
    "        for idx, i, image in zip(range(len(output)), output, X):\n",
    "            # Store true and predicted\n",
    "            y_true.append(int(y[idx]))\n",
    "            y_pred.append(int(torch.argmax(i)))\n",
    "\n",
    "            # Was it labelled correctly?\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                misclassified.append({'image': image, \n",
    "                                      'true_label': int(y[idx]), \n",
    "                                      'predicted_label': int(torch.argmax(i))})\n",
    "        \n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a nice accuracy, but wait! We know from Dr Will Pearse's talk that accuracy is not the best metric to measure how well our model is doing. Let build a confusion matrix instead, which compares how many samples are correctly classified for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = list(range(10)))\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that we are performing well across all classes with slight variations. Let us look at some examples of misclassified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this number to look at different examples\n",
    "ex_number = 5\n",
    "\n",
    "# Some boring plotting code\n",
    "_, ax = plt.subplots()\n",
    "ax.imshow(misclassified[ex_number]['image'].reshape(28,28), cmap=plt.cm.gray_r)\n",
    "ax.set_axis_off()\n",
    "print('True label:', misclassified[ex_number]['true_label'])\n",
    "print('Predicted label:', misclassified[ex_number]['predicted_label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try\n",
    "- As we did in the Neural Network Playground, play around with hyperparameters (network architectures, learning rate, batch size, activation function) and see how this changes the training and the learning curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagepytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
